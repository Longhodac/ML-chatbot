{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71f2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f12868",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35560f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "with open('data/intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = [] \n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c89220e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder (read more about this)\n",
    "# convert categorical text labels into a numerical format \n",
    "\n",
    "# map your words into numbers \n",
    "lbl_encoder = LabelEncoder() \n",
    "\n",
    "# assigns every unique category a unique integer \n",
    "lbl_encoder.fit(training_labels)\n",
    "\n",
    "# swaps the the text labels  for the integers during the fit step\n",
    "training_labels = lbl_encoder.transform(training_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b6ec0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f639c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data using Tokenization method \n",
    "\n",
    "# keeps only the 1000 most frequent words in the dataset. Everything else will be ignored \n",
    "vocab_size = 1000\n",
    "\n",
    "# each word will eventually be represented by a vector of 16 numbers \n",
    "embedding_dim = 1\n",
    "\n",
    "# sets a uniform length for the input. Every sentence will be 20 \"tokens\" long\n",
    "max_len = 20\n",
    "\n",
    "# \"out of vocabulary\" if the model encounters a word it doesnt recognize, it will replace it with this tag instead of skipping it\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "\n",
    "# scans your dataset and creates a dictionary where every unique word is map to a unique integer\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# save the dictionary \n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# converts the actual sentences into a list of numbers  \n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "\n",
    "# neural network requires input data to have fixed shape \n",
    "# if its too short then add '0' at the end of the \"pad\"\n",
    "# if its too long then cut off the end of the sentences w the setting \"post\"\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c6c30",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078e2c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
